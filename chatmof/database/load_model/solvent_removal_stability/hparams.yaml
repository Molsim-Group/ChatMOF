config:
  __doc__: '

    # prepare_data

    max_num_atoms = 300

    min_length = 30

    max_length = 60

    radius = 8

    max_nbr_atoms = 12

    '
  accelerator: gpu
  atom_fea_len: 64
  batch_size: 32
  decay_power: 1
  devices: 4
  downstream: ssc
  draw_false_grid: false
  drop_rate: 0.1
  end_lr: 0
  exp_name: downstream_pmtransformer_mof_ssc
  hid_dim: 768
  img_size: 30
  in_chans: 1
  learning_rate: 0.0001
  load_path: /scratch/x2513a02/MOFTransformer/moftransformer/model/new_pmtransformer.ckpt
  log_dir: result_downstream_public
  loss_names:
    bbc: 0
    classification: 1
    ggm: 0
    moc: 0
    mpp: 0
    mtp: 0
    regression: 0
    vfp: 0
  lr_mult: 1
  max_epochs: 20
  max_graph_len: 300
  max_grid_len: -1
  max_nbr_atoms: 12
  max_steps: -1
  mean: 0.592
  mlp_ratio: 4
  mpp_ratio: 0.15
  n_classes: 2
  nbr_fea_len: 64
  num_heads: 12
  num_layers: 12
  num_nodes: 1
  num_workers: 16
  optim_type: adamw
  patch_size: 5
  per_gpu_batchsize: 8
  precision: 16
  resume_from: null
  root_dataset: /scratch/x2513a02/data/downstream_public/6_textmining_stability/ssc
  seed: 0
  std: 0.491
  test_only: false
  val_check_interval: 1.0
  visualize: false
  warmup_steps: 0.05
  weight_decay: 0.01
